#!/usr/bin/env python3
"""
AudioAI Organizer - Intelligent Audio Library Organization

Transform your chaotic audio collections into intelligently organized, 
searchable libraries with AI that actually listens to your content.

Usage:
    from audioai_organizer import AdaptiveAudioOrganizer
    
    organizer = AdaptiveAudioOrganizer(
        openai_api_key="your-api-key",
        base_directory="/path/to/audio/library"
    )
    
    # Process files interactively
    organizer.process_file_interactive("audio.mp3")
    
    # Batch process with smart interaction
    organizer.interactive_batch_process(file_list)

Author: Built with ‚ù§Ô∏è for the audio community
License: MIT
"""

import os
import shutil
import json
import time
import hashlib
import re
from pathlib import Path
from datetime import datetime
from collections import defaultdict
import pickle

# Core dependencies
import librosa
import numpy as np
import mutagen
import pandas as pd
from openai import OpenAI

# Optional dependencies
try:
    import IPython.display as ipd
    JUPYTER_AVAILABLE = True
except ImportError:
    JUPYTER_AVAILABLE = False

try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False


class AdaptiveAudioOrganizer:
    """
    Intelligent audio library organizer with AI-powered classification,
    interactive learning, and adaptive organization patterns.
    """
    
    def __init__(self, openai_api_key, base_directory):
        """
        Initialize the AudioAI Organizer.
        
        Args:
            openai_api_key (str): Your OpenAI API key
            base_directory (str): Base path for your audio library
        """
        self.client = OpenAI(api_key=openai_api_key)
        self.base_dir = Path(base_directory)
        
        # Learning system files
        self.learning_data_file = self.base_dir / "04_METADATA_SYSTEM" / "learning_data.pkl"
        self.discovered_categories_file = self.base_dir / "04_METADATA_SYSTEM" / "discovered_categories.json"
        
        # Load existing learning data
        self.learning_data = self.load_learning_data()
        self.discovered_categories = self.load_discovered_categories()
        
        # Base categories that can expand
        self.base_categories = {
            "music_ambient": ["contemplative", "tension_building", "wonder_discovery", "melancholic", "mysterious"],
            "sfx_consciousness": ["subtle_background", "narrative_support", "dramatic_punctuation"],
            "sfx_human": ["subtle_background", "narrative_support", "dramatic_punctuation"],
            "sfx_environmental": ["subtle_background", "narrative_support", "dramatic_punctuation"],
            "sfx_technology": ["subtle_background", "narrative_support", "dramatic_punctuation"],
            "voice_element": ["contemplative", "melancholic", "mysterious", "dramatic_punctuation"],
        }
        
        # Dynamic folder mapping
        self.folder_map = self.build_dynamic_folder_map()
        self.audio_extensions = {'.mp3', '.wav', '.aiff', '.m4a', '.flac', '.ogg', '.wma'}
        
        # Default interaction threshold
        self.interaction_threshold = 0.7
        
        # Create base directory structure
        self.create_directory_structure()
        
        print(f"üéµ AudioAI Organizer initialized!")
        print(f"üìÅ Base directory: {self.base_dir}")
        print(f"üß† Learning data: {len(self.learning_data['classifications'])} files processed")
    
    def create_directory_structure(self):
        """Create the base directory structure."""
        directories = [
            "01_UNIVERSAL_ASSETS/MUSIC_LIBRARY/by_mood",
            "01_UNIVERSAL_ASSETS/SFX_LIBRARY/by_category",
            "01_UNIVERSAL_ASSETS/VOICE_ELEMENTS",
            "THEMATIC_COLLECTIONS",
            "04_METADATA_SYSTEM",
            "TO_SORT"
        ]
        
        for directory in directories:
            (self.base_dir / directory).mkdir(parents=True, exist_ok=True)
    
    def load_learning_data(self):
        """Load historical classification data."""
        if self.learning_data_file.exists():
            with open(self.learning_data_file, 'rb') as f:
                return pickle.load(f)
        return {
            'classifications': [],
            'user_corrections': [],
            'patterns': defaultdict(list),
            'filename_patterns': defaultdict(list)
        }
    
    def save_learning_data(self):
        """Save learning data for future use."""
        self.learning_data_file.parent.mkdir(parents=True, exist_ok=True)
        with open(self.learning_data_file, 'wb') as f:
            pickle.dump(self.learning_data, f)
    
    def load_discovered_categories(self):
        """Load dynamically discovered categories."""
        if self.discovered_categories_file.exists():
            with open(self.discovered_categories_file, 'r') as f:
                return json.load(f)
        return {
            'new_categories': [],
            'new_moods': [],
            'frequency_counts': defaultdict(int)
        }
    
    def save_discovered_categories(self):
        """Save discovered categories."""
        self.discovered_categories_file.parent.mkdir(parents=True, exist_ok=True)
        with open(self.discovered_categories_file, 'w') as f:
            json.dump(self.discovered_categories, f, indent=2)
    
    def build_dynamic_folder_map(self):
        """Build folder mapping including discovered categories."""
        base_map = {
            "music_ambient + contemplative": "01_UNIVERSAL_ASSETS/MUSIC_LIBRARY/by_mood/contemplative/",
            "music_ambient + tension_building": "01_UNIVERSAL_ASSETS/MUSIC_LIBRARY/by_mood/tension_building/",
            "music_ambient + wonder_discovery": "01_UNIVERSAL_ASSETS/MUSIC_LIBRARY/by_mood/wonder_discovery/",
            "music_ambient + melancholic": "01_UNIVERSAL_ASSETS/MUSIC_LIBRARY/by_mood/melancholic/",
            "music_ambient + mysterious": "01_UNIVERSAL_ASSETS/MUSIC_LIBRARY/by_mood/mysterious/",
            "sfx_consciousness + subtle_background": "01_UNIVERSAL_ASSETS/SFX_LIBRARY/by_category/consciousness/thought_processing/",
            "sfx_consciousness + narrative_support": "01_UNIVERSAL_ASSETS/SFX_LIBRARY/by_category/consciousness/awakening_emergence/",
            "sfx_consciousness + dramatic_punctuation": "01_UNIVERSAL_ASSETS/SFX_LIBRARY/by_category/consciousness/memory_formation/",
            "sfx_human + subtle_background": "01_UNIVERSAL_ASSETS/SFX_LIBRARY/by_category/human_elements/breathing_heartbeat/",
            "sfx_human + narrative_support": "01_UNIVERSAL_ASSETS/SFX_LIBRARY/by_category/human_elements/emotional_responses/",
            "sfx_human + dramatic_punctuation": "01_UNIVERSAL_ASSETS/SFX_LIBRARY/by_category/human_elements/environmental_human/",
            "sfx_environmental + subtle_background": "01_UNIVERSAL_ASSETS/SFX_LIBRARY/by_category/abstract_conceptual/time_space/",
            "sfx_environmental + narrative_support": "01_UNIVERSAL_ASSETS/SFX_LIBRARY/by_category/abstract_conceptual/transformation/",
            "sfx_environmental + dramatic_punctuation": "01_UNIVERSAL_ASSETS/SFX_LIBRARY/by_category/abstract_conceptual/connection_bridging/",
            "sfx_technology + subtle_background": "01_UNIVERSAL_ASSETS/SFX_LIBRARY/by_category/technology/digital_processing/",
            "sfx_technology + narrative_support": "01_UNIVERSAL_ASSETS/SFX_LIBRARY/by_category/technology/communication/",
            "sfx_technology + dramatic_punctuation": "01_UNIVERSAL_ASSETS/SFX_LIBRARY/by_category/technology/system_alerts/",
            "voice_element + contemplative": "01_UNIVERSAL_ASSETS/VOICE_ELEMENTS/narrator_banks/",
            "voice_element + melancholic": "01_UNIVERSAL_ASSETS/VOICE_ELEMENTS/processed_vocals/",
            "voice_element + mysterious": "01_UNIVERSAL_ASSETS/VOICE_ELEMENTS/vocal_textures/",
            "voice_element + dramatic_punctuation": "01_UNIVERSAL_ASSETS/VOICE_ELEMENTS/character_voices/",
            "default": "TO_SORT/"
        }
        
        # Add discovered categories
        for category in self.discovered_categories.get('new_categories', []):
            for mood in self.discovered_categories.get('new_moods', []):
                key = f"{category} + {mood}"
                if key not in base_map:
                    if category.startswith('music_'):
                        base_map[key] = f"01_UNIVERSAL_ASSETS/MUSIC_LIBRARY/by_mood/{mood}/"
                    elif category.startswith('sfx_'):
                        sfx_type = category.replace('sfx_', '')
                        base_map[key] = f"01_UNIVERSAL_ASSETS/SFX_LIBRARY/by_category/{sfx_type}/{mood}/"
                    elif category.startswith('voice_'):
                        base_map[key] = f"01_UNIVERSAL_ASSETS/VOICE_ELEMENTS/{mood}/"
                    else:
                        base_map[key] = f"01_UNIVERSAL_ASSETS/EXPERIMENTAL/{category}/{mood}/"
        
        return base_map
    
    def get_audio_metadata(self, file_path):
        """Extract audio metadata using mutagen."""
        try:
            audio_file = mutagen.File(file_path)
            if audio_file is not None:
                duration = audio_file.info.length
                return {
                    'duration': f"{int(duration // 60)}:{int(duration % 60):02d}",
                    'duration_seconds': duration,
                    'bitrate': getattr(audio_file.info, 'bitrate', 'Unknown'),
                    'sample_rate': getattr(audio_file.info, 'sample_rate', 'Unknown')
                }
        except Exception as e:
            print(f"Could not read metadata for {file_path}: {e}")
        return {'duration': 'Unknown', 'duration_seconds': 0, 'bitrate': 'Unknown', 'sample_rate': 'Unknown'}
    
    def analyze_audio_content(self, file_path):
        """Enhanced audio analysis with advanced voice detection - COMPLETE VERSION."""
        try:
            print(f"  üéµ Analyzing audio content...")
            
            # Load first 30 seconds for efficiency
            y, sr = librosa.load(file_path, duration=30)
            
            features = {}
            
            # Spectral brightness
            spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
            features['spectral_centroid_mean'] = float(np.mean(spectral_centroids))
            
            # Spectral rolloff (high frequency content)
            spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]
            features['spectral_rolloff_mean'] = float(np.mean(spectral_rolloff))
            
            # Zero crossing rate (noisiness vs tonality)
            zcr = librosa.feature.zero_crossing_rate(y)[0]
            features['zero_crossing_rate'] = float(np.mean(zcr))
            
            # Tempo analysis - FIXED for proper conversion
            tempo, beats = librosa.beat.beat_track(y=y, sr=sr)
            features['tempo'] = float(tempo) if np.isscalar(tempo) else float(tempo[0])
            
            # Energy (RMS)
            rms = librosa.feature.rms(y=y)[0]
            features['rms_energy'] = float(np.mean(rms))
            
            # Onset detection (rhythmic activity)
            onset_frames = librosa.onset.onset_detect(y=y, sr=sr)
            features['onset_rate'] = len(onset_frames) / (len(y) / sr)
            
            # Spectral contrast (timbral texture)
            contrast = librosa.feature.spectral_contrast(y=y, sr=sr)
            features['spectral_contrast_mean'] = float(np.mean(contrast))
            
            # ENHANCED VOICE-SPECIFIC FEATURES
            # =================================
            
            # 1. MFCC (Mel-frequency cepstral coefficients) - excellent for voice
            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
            features['mfcc_mean'] = float(np.mean(mfccs))
            features['mfcc_var'] = float(np.var(mfccs))
            
            # 2. Formant-like analysis using spectral peaks
            # Voice has distinct formant patterns
            stft = librosa.stft(y)
            magnitude = np.abs(stft)
            freq_peaks = []
            for frame in magnitude.T[:100]:  # Sample first 100 frames
                peaks = np.where(frame > np.percentile(frame, 85))[0]  # Top 15% of frequencies
                if len(peaks) > 0:
                    freq_peaks.extend(peaks)
            
            if freq_peaks:
                features['formant_concentration'] = float(np.std(freq_peaks))  # Voice has clustered formants
                features['formant_peak_count'] = len(freq_peaks) / 100  # Normalized
            else:
                features['formant_concentration'] = 0.0
                features['formant_peak_count'] = 0.0
            
            # 3. Harmonic-to-noise ratio (voice is more harmonic)
            harmonic = librosa.effects.harmonic(y)
            percussive = librosa.effects.percussive(y)
            harmonic_energy = np.sum(harmonic**2)
            percussive_energy = np.sum(percussive**2)
            
            if percussive_energy > 0:
                features['harmonic_ratio'] = float(harmonic_energy / (harmonic_energy + percussive_energy))
            else:
                features['harmonic_ratio'] = 1.0
            
            # 4. Spectral bandwidth (voice has characteristic bandwidth)
            bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)[0]
            features['spectral_bandwidth_mean'] = float(np.mean(bandwidth))
            
            # 5. Pitch stability (voice has more stable pitch than music/SFX)
            pitches, magnitudes = librosa.piptrack(y=y, sr=sr)
            pitch_values = []
            for t in range(pitches.shape[1]):
                index = magnitudes[:, t].argmax()
                pitch = pitches[index, t]
                if pitch > 0:  # Valid pitch
                    pitch_values.append(pitch)
            
            if len(pitch_values) > 1:
                features['pitch_stability'] = float(1.0 / (1.0 + np.std(pitch_values)))  # Higher = more stable
                features['pitch_presence'] = len(pitch_values) / pitches.shape[1]  # How much has detectable pitch
            else:
                features['pitch_stability'] = 0.0
                features['pitch_presence'] = 0.0
            
            # Interpret features into human-readable descriptions
            interpretation = self.interpret_audio_features(features)
            
            return {
                'raw_features': features,
                'interpretation': interpretation
            }
            
        except Exception as e:
            print(f"  ‚ö†Ô∏è  Audio analysis failed: {e}")
            return None
    
    def interpret_audio_features(self, features):
        """Enhanced interpretation with advanced voice detection."""
        interpretation = {}
        
        # Basic audio characteristics
        if features['spectral_centroid_mean'] > 3000:
            interpretation['brightness'] = 'bright'
        elif features['spectral_centroid_mean'] > 1000:
            interpretation['brightness'] = 'balanced'
        else:
            interpretation['brightness'] = 'dark'
        
        if features['zero_crossing_rate'] > 0.1:
            interpretation['texture'] = 'noisy'
        elif features['zero_crossing_rate'] > 0.05:
            interpretation['texture'] = 'moderate'
        else:
            interpretation['texture'] = 'tonal'
        
        if features['rms_energy'] > 0.1:
            interpretation['energy'] = 'high'
        elif features['rms_energy'] > 0.05:
            interpretation['energy'] = 'medium'
        else:
            interpretation['energy'] = 'low'
        
        if features['tempo'] > 120:
            interpretation['pace'] = 'fast'
        elif features['tempo'] > 80:
            interpretation['pace'] = 'moderate'
        else:
            interpretation['pace'] = 'slow'
        
        if features['onset_rate'] > 2:
            interpretation['rhythmic_activity'] = 'high'
        elif features['onset_rate'] > 0.5:
            interpretation['rhythmic_activity'] = 'moderate'
        else:
            interpretation['rhythmic_activity'] = 'low'
        
        # ENHANCED VOICE DETECTION ALGORITHM
        # ==================================
        
        voice_score = 0
        voice_indicators = []
        
        # 1. MFCC patterns (voice has distinctive MFCC patterns)
        if 5 < features.get('mfcc_mean', 0) < 15:  # Typical voice MFCC range
            voice_score += 2
            voice_indicators.append("mfcc_pattern")
        
        # 2. Harmonic content (voice is highly harmonic)
        if features.get('harmonic_ratio', 0) > 0.7:
            voice_score += 3
            voice_indicators.append("high_harmonic_content")
        
        # 3. Pitch stability (voice has stable pitch)
        if features.get('pitch_stability', 0) > 0.3:
            voice_score += 2
            voice_indicators.append("stable_pitch")
        
        # 4. Pitch presence (voice has consistent pitch)
        if features.get('pitch_presence', 0) > 0.4:
            voice_score += 2
            voice_indicators.append("consistent_pitch")
        
        # 5. Spectral characteristics
        if 1000 < features['spectral_centroid_mean'] < 4000:  # Voice frequency range
            voice_score += 1
            voice_indicators.append("voice_frequency_range")
        
        # 6. Formant concentration (voice has clustered formants)
        if features.get('formant_concentration', 0) > 50:
            voice_score += 1
            voice_indicators.append("formant_structure")
        
        # 7. Low rhythmic activity (voice is less rhythmic than music)
        if features['onset_rate'] < 1.0:
            voice_score += 1
            voice_indicators.append("low_rhythmic_activity")
        
        # 8. Spectral bandwidth (voice has characteristic bandwidth)
        if features.get('spectral_bandwidth_mean', 0) < 2000:  # Voice typically has narrower bandwidth
            voice_score += 1
            voice_indicators.append("narrow_bandwidth")
        
        # Determine voice likelihood with confidence levels
        if voice_score >= 7:
            interpretation['content_type'] = 'voice_high_confidence'
            interpretation['voice_probability'] = 'very_likely'
            interpretation['voice_confidence'] = 0.9
        elif voice_score >= 5:
            interpretation['content_type'] = 'voice_medium_confidence'
            interpretation['voice_probability'] = 'likely'
            interpretation['voice_confidence'] = 0.7
        elif voice_score >= 3:
            interpretation['content_type'] = 'voice_low_confidence'
            interpretation['voice_probability'] = 'possible'
            interpretation['voice_confidence'] = 0.4
        else:
            interpretation['content_type'] = 'non_voice'
            interpretation['voice_probability'] = 'unlikely'
            interpretation['voice_confidence'] = 0.1
        
        interpretation['voice_indicators'] = voice_indicators
        interpretation['voice_score'] = voice_score
        
        return interpretation
    
    def analyze_filename_patterns(self, filename):
        """Analyze filename for semantic patterns."""
        filename_lower = filename.lower()
        patterns = []
        
        descriptors = {
            'mood': ['contemplative', 'mysterious', 'dark', 'bright', 'tense', 'calm', 'energetic'],
            'instrument': ['piano', 'guitar', 'synth', 'drum', 'bass', 'vocal', 'strings'],
            'voice': ['male', 'female', 'voice', 'vocal', 'speech', 'talk', 'dialogue'],
            'nature': ['nature', 'wind', 'rain', 'water', 'forest', 'bird'],
            'mechanical': ['mech', 'robot', 'machine', 'tech', 'digital', 'synth'],
            'emotional': ['sad', 'happy', 'dark', 'bright', 'calm', 'tense'],
            'temporal': ['intro', 'outro', 'loop', 'oneshot', 'sustained']
        }
        
        for category, keywords in descriptors.items():
            if any(keyword in filename_lower for keyword in keywords):
                patterns.append(category)
        
        return patterns
    
    def find_similar_files(self, filename):
        """Find similar files from learning data."""
        similar_files = []
        
        for classification in self.learning_data['classifications']:
            if classification.get('filename'):
                # Simple similarity based on common words
                filename_words = set(re.findall(r'\w+', filename.lower()))
                other_words = set(re.findall(r'\w+', classification['filename'].lower()))
                
                common_words = filename_words.intersection(other_words)
                if len(common_words) >= 2:  # At least 2 words in common
                    similar_files.append(classification)
        
        return similar_files[:5]  # Return top 5
    
    def build_adaptive_prompt(self, file_path, metadata, user_description=""):
        """Build a prompt that learns from previous classifications."""
        
        # Analyze filename patterns
        filename_patterns = self.analyze_filename_patterns(file_path.name)
        
        # Get historical context
        similar_files = self.find_similar_files(file_path.name)
        
        # Build dynamic categories list
        all_categories = list(self.base_categories.keys()) + self.discovered_categories.get('new_categories', [])
        all_moods = []
        for cat_moods in self.base_categories.values():
            all_moods.extend(cat_moods)
        all_moods.extend(self.discovered_categories.get('new_moods', []))
        all_moods = list(set(all_moods))  # Remove duplicates
        
        file_stats = os.stat(file_path)
        file_size_mb = file_stats.st_size / (1024 * 1024)
        
        # Get audio analysis if possible
        audio_analysis = self.analyze_audio_content(file_path)
        audio_description = ""
        if audio_analysis:
            interp = audio_analysis['interpretation']
            features = audio_analysis['raw_features']
            audio_description = f"""
AUDIO ANALYSIS:
- Brightness: {interp.get('brightness', 'unknown')}
- Energy: {interp.get('energy', 'unknown')}  
- Texture: {interp.get('noisiness', 'unknown')}
- Tempo: {features.get('tempo', 0):.1f} BPM
- Rhythmic Activity: {interp.get('rhythmic_activity', 'unknown')}
"""
        
        context = ""
        if similar_files:
            context = f"\nCONTEXT from similar files:\n"
            for similar in similar_files[:3]:
                context += f"- {similar['filename']}: {similar['category']} + {similar['mood']}\n"
        
        if filename_patterns:
            context += f"\nFILENAME PATTERNS detected: {', '.join(filename_patterns)}\n"
        
        if user_description:
            context += f"\nUSER DESCRIPTION: {user_description}\n"
        
        prompt = f"""You are an expert audio librarian with adaptive learning capabilities. Analyze this audio file and classify it, considering both standard categories and potentially discovering new ones.

FILE DETAILS:
- Filename: {file_path.name}
- Size: {file_size_mb:.1f} MB
- Duration: {metadata.get('duration', 'Unknown')}
- Format: {file_path.suffix.upper()}
{audio_description}
{context}

KNOWN CATEGORIES: {', '.join(all_categories)}
KNOWN MOODS: {', '.join(all_moods)}

CLASSIFICATION TASK:
Analyze this audio file and provide a JSON response with:
1. category: Main type (from known categories or suggest new one starting with music_, sfx_, or voice_)
2. mood: Emotional/functional context (from known moods or suggest new one)
3. intensity: narrative impact level (subtle_background, narrative_support, dramatic_punctuation)
4. energy_level: 1-10 scale
5. tags: list of descriptive keywords
6. bpm: tempo if musical (0 if not applicable)
7. suggested_filename: semantic name preserving meaning + metadata (without extension)
8. confidence: 0-1 how certain you are
9. reasoning: brief explanation of classification
10. discovered_elements: list any new categories/moods you're suggesting

Focus on understanding the creative and emotional context. Be willing to discover new categories that make sense for this library.

Respond with valid JSON only:"""
        
        return prompt
    
    def classify_audio_file(self, file_path, user_description=""):
        """Classify with adaptive learning."""
        
        metadata = self.get_audio_metadata(file_path)
        prompt = self.build_adaptive_prompt(file_path, metadata, user_description)
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.4  # Slightly higher for more creativity
            )
            
            raw_response = response.choices[0].message.content.strip()
            
            # Handle markdown-wrapped JSON
            if raw_response.startswith('```json'):
                raw_response = raw_response.replace('```json', '').replace('```', '').strip()
            
            classification = json.loads(raw_response)
            
            # Learn from this classification
            self.learn_from_classification(file_path, classification)
            
            return classification
            
        except Exception as e:
            print(f"Classification failed: {e}")
            return None
    
    def learn_from_classification(self, file_path, classification):
        """Learn patterns from successful classifications - FIXED VERSION."""
        if not classification:
            return
        
        # Store classification
        learning_entry = {
            'filename': file_path.name,
            'classification': classification,
            'timestamp': datetime.now().isoformat(),
            'file_path': str(file_path)
        }
        self.learning_data['classifications'].append(learning_entry)
        
        # Track new categories/moods - FIXED logic
        category = classification.get('category', '')
        mood = classification.get('mood', '')
        
        # Check for new categories - FIXED
        if category and category not in self.base_categories:
            if category not in self.discovered_categories['new_categories']:
                self.discovered_categories['new_categories'].append(category)
                print(f"üÜï Discovered new category: {category}")
        
        # Check for new moods - FIXED logic  
        all_base_moods = []
        for moods in self.base_categories.values():
            all_base_moods.extend(moods)
        
        if mood and mood not in all_base_moods:
            if mood not in self.discovered_categories['new_moods']:
                self.discovered_categories['new_moods'].append(mood)
                print(f"üÜï Discovered new mood: {mood}")
        
        # Update frequency counts
        if category and mood:
            self.discovered_categories['frequency_counts'][f"{category}+{mood}"] += 1
        
        # Learn filename patterns
        filename_patterns = self.analyze_filename_patterns(file_path.name)
        for pattern in filename_patterns:
            self.learning_data['filename_patterns'][pattern].append({
                'category': category,
                'mood': mood,
                'filename': file_path.name
            })
        
        # Save learning data
        try:
            self.save_learning_data()
            self.save_discovered_categories()
        except Exception as e:
            print(f"Warning: Could not save learning data: {e}")
        
        # Rebuild folder map with new discoveries
        self.folder_map = self.build_dynamic_folder_map()
    
    def generate_enhanced_filename(self, file_path, classification, audio_analysis):
        """Generate enhanced filename - FIXED for encoding issues."""
        
        # Extract key info
        category = classification.get('category', 'unknown')
        mood = classification.get('mood', 'unknown')
        intensity = classification.get('intensity', 'unknown')
        energy = classification.get('energy_level', 0)
        
        # Get audio characteristics
        bpm = None
        brightness = None
        texture = None
        
        if audio_analysis:
            raw_features = audio_analysis.get('raw_features', {})
            interpretation = audio_analysis.get('interpretation', {})
            
            bpm = raw_features.get('tempo', 0)
            brightness = interpretation.get('brightness', '')
            texture = interpretation.get('texture', '')
        
        # Build filename components
        components = []
        
        # 1. Category prefix
        if category.startswith('music_'):
            components.append('MUS')
        elif category.startswith('sfx_'):
            sfx_type = category.replace('sfx_', '').upper()
            components.append(f'SFX_{sfx_type[:4]}')
        elif category.startswith('voice_'):
            components.append('VOX')
        else:
            components.append(category.upper()[:6])
        
        # 2. BPM (if detected and makes sense)
        if bpm and bpm > 30 and bpm < 300:
            components.append(f"{int(bpm)}bpm")
        
        # 3. Mood/Energy
        mood_short = mood[:4].upper() if mood else ""
        if mood_short:
            components.append(mood_short)
        
        # 4. Energy level
        if energy:
            components.append(f"E{energy}")
        
        # 5. Audio characteristics
        if brightness in ['bright', 'dark']:
            components.append(brightness[:3].upper())
        
        if texture == 'tonal':
            components.append('TON')
        elif texture == 'noisy':
            components.append('NOI')
        
        # 6. Intensity indicator
        intensity_map = {
            'subtle_background': 'BG',
            'narrative_support': 'SUP', 
            'dramatic_punctuation': 'DRA'
        }
        if intensity in intensity_map:
            components.append(intensity_map[intensity])
        
        # Join components
        new_filename = '_'.join(components)
        
        # Keep original extension
        original_ext = file_path.suffix
        
        # Add uniqueness - FIXED encoding issue
        file_hash = hashlib.md5(str(file_path).encode('utf-8')).hexdigest()[:4]
        
        final_filename = f"{new_filename}_{file_hash}{original_ext}"
        
        return final_filename
    
    def create_metadata_spreadsheet(self):
        """Create comprehensive metadata spreadsheet."""
        print("üìä Creating comprehensive metadata spreadsheet...")
        
        data = []
        
        for entry in self.learning_data['classifications']:
            classification = entry.get('classification', {})
            audio_analysis = classification.get('audio_analysis', {})
            
            # Extract all the metadata
            row = {
                # Original file info
                'original_filename': entry.get('filename', ''),
                'original_path': entry.get('file_path', ''),
                'processed_date': entry.get('timestamp', ''),
                
                # Classification data
                'category': classification.get('category', ''),
                'mood': classification.get('mood', ''),
                'intensity': classification.get('intensity', ''),
                'energy_level': classification.get('energy_level', 0),
                'confidence': classification.get('confidence', 0),
                'suggested_filename': classification.get('suggested_filename', ''),
                'target_folder': self.determine_target_folder(classification),
                
                # Audio analysis data
                'bpm': None,
                'brightness': '',
                'texture': '',
                'audio_energy': '',
                'pace': '',
                'rhythmic_activity': '',
                'spectral_centroid': None,
                'zero_crossing_rate': None,
                'rms_energy': None,
                
                # Metadata
                'tags': ', '.join(classification.get('tags', [])),
                'thematic_notes': classification.get('thematic_notes', ''),
                'reasoning': classification.get('reasoning', ''),
                'discovered_elements': ', '.join(classification.get('discovered_elements', [])),
                'audio_insights': classification.get('audio_insights', '')
            }
            
            # Add audio analysis details if available
            if audio_analysis:
                raw_features = audio_analysis.get('raw_features', {})
                interpretation = audio_analysis.get('interpretation', {})
                
                row.update({
                    'bpm': raw_features.get('tempo', None),
                    'brightness': interpretation.get('brightness', ''),
                    'texture': interpretation.get('texture', ''),
                    'audio_energy': interpretation.get('energy', ''),
                    'pace': interpretation.get('pace', ''),
                    'rhythmic_activity': interpretation.get('rhythmic_activity', ''),
                    'spectral_centroid': raw_features.get('spectral_centroid_mean', None),
                    'zero_crossing_rate': raw_features.get('zero_crossing_rate', None),
                    'rms_energy': raw_features.get('rms_energy', None)
                })
            
            data.append(row)
        
        if not data:
            print("‚ö†Ô∏è No classification data found")
            return None, None
        
        # Create DataFrame
        df = pd.DataFrame(data)
        
        # Save to Excel with multiple sheets
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        spreadsheet_path = self.base_dir / "04_METADATA_SYSTEM" / f"audio_metadata_{timestamp}.xlsx"
        
        try:
            with pd.ExcelWriter(spreadsheet_path, engine='openpyxl') as writer:
                # Main data sheet
                df.to_excel(writer, sheet_name='All_Files', index=False)
                
                # Summary by category
                if 'category' in df.columns and len(df) > 0:
                    category_summary = df.groupby('category').agg({
                        'original_filename': 'count',
                        'confidence': 'mean',
                        'energy_level': 'mean'
                    }).round(2)
                    category_summary.columns = ['File_Count', 'Avg_Confidence', 'Avg_Energy']
                    category_summary.to_excel(writer, sheet_name='Category_Summary')
                
                # Summary by mood
                if 'mood' in df.columns and len(df) > 0:
                    mood_summary = df.groupby('mood').agg({
                        'original_filename': 'count',
                        'confidence': 'mean',
                        'energy_level': 'mean'
                    }).round(2)
                    mood_summary.columns = ['File_Count', 'Avg_Confidence', 'Avg_Energy']
                    mood_summary.to_excel(writer, sheet_name='Mood_Summary')
                
                # Learning statistics
                learning_stats = pd.DataFrame([{
                    'Total_Files_Processed': len(df),
                    'High_Confidence_Files': len(df[df['confidence'] >= 0.8]) if 'confidence' in df.columns else 0,
                    'User_Corrected_Files': len(df[df.get('user_corrected', False) == True]) if 'user_corrected' in df.columns else 0,
                    'Unique_Categories': df['category'].nunique() if 'category' in df.columns else 0,
                    'Unique_Moods': df['mood'].nunique() if 'mood' in df.columns else 0,
                    'Avg_Overall_Confidence': df['confidence'].mean() if 'confidence' in df.columns else 0
                }])
                learning_stats.to_excel(writer, sheet_name='Learning_Stats', index=False)
            
            print(f"üìà Excel metadata spreadsheet saved: {spreadsheet_path}")
        except ImportError:
            print("‚ö†Ô∏è openpyxl not available, creating CSV instead...")
            return self.create_metadata_csv()
        
        print(f"üìã Sheets: All_Files, Category_Summary, Mood_Summary, Learning_Stats")
        
        return df, spreadsheet_path
    
    def create_metadata_csv(self):
        """Create CSV version of metadata (no extra libraries needed)."""
        
        data = []
        
        for entry in self.learning_data['classifications']:
            classification = entry.get('classification', {})
            audio_analysis = classification.get('audio_analysis', {})
            
            # Extract metadata
            row = {
                'original_filename': entry.get('filename', ''),
                'original_path': entry.get('file_path', ''),
                'processed_date': entry.get('timestamp', ''),
                'category': classification.get('category', ''),
                'mood': classification.get('mood', ''),
                'intensity': classification.get('intensity', ''),
                'energy_level': classification.get('energy_level', 0),
                'confidence': classification.get('confidence', 0),
                'suggested_filename': classification.get('suggested_filename', ''),
                'target_folder': self.determine_target_folder(classification),
                'tags': ', '.join(classification.get('tags', [])),
                'thematic_notes': classification.get('thematic_notes', ''),
                'reasoning': classification.get('reasoning', ''),
                'audio_insights': classification.get('audio_insights', '')
            }
            
            # Add audio analysis if available
            if audio_analysis:
                raw_features = audio_analysis.get('raw_features', {})
                interpretation = audio_analysis.get('interpretation', {})
                
                row.update({
                    'bpm': raw_features.get('tempo', ''),
                    'brightness': interpretation.get('brightness', ''),
                    'texture': interpretation.get('texture', ''),
                    'audio_energy': interpretation.get('energy', ''),
                    'pace': interpretation.get('pace', ''),
                    'rhythmic_activity': interpretation.get('rhythmic_activity', '')
                })
            
            data.append(row)
        
        # Create DataFrame and save as CSV
        df = pd.DataFrame(data)
        
        # Save CSV
        csv_file = self.base_dir / "04_METADATA_SYSTEM" / f"audio_metadata_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        csv_file.parent.mkdir(parents=True, exist_ok=True)
        
        df.to_csv(csv_file, index=False)
        
        print(f"üìÑ CSV metadata created: {csv_file}")
        print(f"üìà Contains {len(df)} files with analysis data")
        
        return df, csv_file
    
    def determine_target_folder(self, classification):
        """Determine target folder, creating new ones if needed."""
        if not classification:
            return self.folder_map["default"]
        
        category = classification.get('category', '')
        mood = classification.get('mood', '')
        intensity = classification.get('intensity', '')
        
        # Try exact match first
        key = f"{category} + {mood}"
        if key in self.folder_map:
            return self.folder_map[key]
        
        # Try with intensity
        key = f"{category} + {intensity}"
        if key in self.folder_map:
            return self.folder_map[key]
        
        # Create new folder path for discovered categories
        if category.startswith('music_'):
            new_path = f"01_UNIVERSAL_ASSETS/MUSIC_LIBRARY/by_mood/{mood}/"
        elif category.startswith('sfx_'):
            sfx_type = category.replace('sfx_', '')
            new_path = f"01_UNIVERSAL_ASSETS/SFX_LIBRARY/by_category/{sfx_type}/{mood}/"
        elif category.startswith('voice_'):
            new_path = f"01_UNIVERSAL_ASSETS/VOICE_ELEMENTS/{mood}/"
        else:
            new_path = f"01_UNIVERSAL_ASSETS/EXPERIMENTAL/{category}/{mood}/"
        
        # Add to folder map
        self.folder_map[key] = new_path
        print(f"üÜï Created new folder mapping: {key} ‚Üí {new_path}")
        
        return new_path
    
    def move_file_safely(self, source_path, target_folder, suggested_filename=""):
        """Move file with safe handling of duplicates."""
        source_path = Path(source_path)
        full_target_dir = self.base_dir / target_folder
        full_target_dir.mkdir(parents=True, exist_ok=True)
        
        # Use suggested filename or original
        filename = suggested_filename if suggested_filename else source_path.stem
        
        # Ensure proper extension
        if not filename.endswith(source_path.suffix):
            filename = f"{filename}{source_path.suffix}"
        
        target_path = full_target_dir / filename
        
        # Handle duplicates with versioning
        counter = 1
        while target_path.exists():
            name_part = filename.replace(source_path.suffix, '')
            target_path = full_target_dir / f"{name_part}_v{counter}{source_path.suffix}"
            counter += 1
        
        try:
            shutil.move(str(source_path), str(target_path))
            return target_path
        except Exception as e:
            print(f"Failed to move {source_path} to {target_path}: {e}")
            return None
    
    def set_interaction_mode(self, mode):
        """Set interaction mode and threshold."""
        mode_thresholds = {
            'never': 0.0,      # No questions, fully automatic
            'minimal': 0.4,    # Only very uncertain files
            'smart': 0.7,      # Ask when uncertain (recommended)
            'always': 1.0      # Always ask
        }
        
        if mode in mode_thresholds:
            self.interaction_threshold = mode_thresholds[mode]
            print(f"üéØ Interaction mode set to '{mode}' (threshold: {self.interaction_threshold})")
        else:
            print(f"‚ùå Unknown mode '{mode}'. Available: {list(mode_thresholds.keys())}")
    
    def should_ask_user(self, classification):
        """Determine if user input is needed based on confidence."""
        if not classification:
            return True
        
        confidence = classification.get('confidence', 0)
        return confidence < self.interaction_threshold
    
    def play_audio_if_possible(self, file_path):
        """Play audio in Jupyter environments."""
        if JUPYTER_AVAILABLE:
            try:
                return ipd.Audio(str(file_path))
            except:
                pass
        return None
    
    def play_audio_clip(self, file_path, duration=15):
        """Play a short clip of the audio file for classification help."""
        try:
            print(f"üéµ Playing {duration}-second preview of: {file_path.name}")
            
            if JUPYTER_AVAILABLE:
                # For Jupyter notebooks, we can use IPython's Audio display
                audio_widget = ipd.Audio(str(file_path), autoplay=False)
                
                # Display the audio player
                ipd.display(audio_widget)
                
                print("üéß Use the player above to listen while answering questions!")
                return True
            else:
                print("üí° Try opening the file manually to listen while classifying")
                return False
                
        except Exception as e:
            print(f"‚ùå Could not play audio: {e}")
            print("üí° Try opening the file manually to listen while classifying")
            return False
    
    def enhanced_ask_classification_questions(self, file_path, initial_classification):
        """Ask questions with audio playback support."""
        
        print(f"\nüìù Quick questions to improve classification:")
        
        # Play audio clip first!
        print(f"\nüéµ Let's listen to the audio first...")
        self.play_audio_clip(file_path, duration=15)
        
        user_input = {}
        
        # Question 1: Content type if unclear
        if not initial_classification or initial_classification.get('confidence', 0) < 0.5:
            print(f"\n1. After listening, what type of content is this?")
            print(f"   a) Music/Musical piece")
            print(f"   b) Sound effect/SFX") 
            print(f"   c) Voice/Speech")
            print(f"   d) Ambient/Atmospheric")
            
            content_type = input("   Your choice (a/b/c/d or description): ").strip().lower()
            user_input['content_type'] = content_type
        
        # Question 2: Mood/feeling (now with audio context!)
        print(f"\n2. After listening, what mood or feeling does this convey?")
        print(f"   (e.g., mysterious, contemplative, tense, playful, eerie, hopeful, etc.)")
        mood_input = input("   Mood: ").strip()
        if mood_input:
            user_input['mood'] = mood_input
        
        # Question 3: How would you use this?
        print(f"\n3. How would you use this in your AI consciousness project?")
        print(f"   a) Background atmosphere")
        print(f"   b) Supporting narrative moments") 
        print(f"   c) Dramatic emphasis/punctuation")
        
        usage = input("   Usage (a/b/c or description): ").strip()
        if usage:
            user_input['usage'] = usage
        
        # Question 4: Any specific tags or themes?
        print(f"\n4. Any specific themes this evokes? (consciousness, memory, digital, organic, etc.)")
        tags_input = input("   Tags: ").strip()
        if tags_input:
            user_input['tags'] = [tag.strip() for tag in tags_input.split(',')]
        
        # Question 5: Semantic description for filename
        print(f"\n5. How would you describe this for the filename?")
        print(f"   (e.g., 'eerie digital pulse', 'warm contemplative pad', 'glitchy consciousness')")
        semantic_input = input("   Description: ").strip()
        if semantic_input:
            user_input['semantic_description'] = semantic_input
        
        print(f"\n   Thanks! That audio context really helps! üéØ")
        
        return user_input
    
    def ask_user_for_clarification(self, file_path, classification):
        """Ask user for input on uncertain classifications."""
        print(f"\nü§î Need your input for: {file_path.name}")
        
        # Play audio if in Jupyter
        audio_widget = self.play_audio_clip(file_path)
        
        if classification:
            print(f"ü§ñ AI suggests:")
            print(f"   Category: {classification.get('category', 'unknown')}")
            print(f"   Mood: {classification.get('mood', 'unknown')}")
            print(f"   Confidence: {classification.get('confidence', 0):.1%}")
            print(f"   Reasoning: {classification.get('reasoning', 'No reasoning provided')}")
        
        print("\nOptions:")
        print("1. Accept AI suggestion")
        print("2. Modify classification")
        print("3. Enhanced interactive questions (with audio)")
        print("4. Skip this file")
        
        choice = input("Your choice (1-4): ").strip()
        
        if choice == "1":
            return classification
        elif choice == "2":
            return self.modify_classification(classification)
        elif choice == "3":
            # Use enhanced questioning with audio playback
            user_responses = self.enhanced_ask_classification_questions(file_path, classification)
            return self.integrate_user_responses(classification, user_responses)
        else:
            return None
    
    def integrate_user_responses(self, initial_classification, user_responses):
        """Integrate user responses into classification."""
        if not initial_classification:
            initial_classification = {}
        
        enhanced_classification = initial_classification.copy()
        
        # Map user responses to classification fields
        if 'content_type' in user_responses:
            content_map = {
                'a': 'music_ambient',
                'b': 'sfx_general', 
                'c': 'voice_element',
                'd': 'sfx_ambient'
            }
            user_type = user_responses['content_type']
            enhanced_classification['category'] = content_map.get(user_type, user_type)
        
        if 'mood' in user_responses:
            enhanced_classification['mood'] = user_responses['mood']
        
        if 'usage' in user_responses:
            usage_map = {
                'a': 'subtle_background',
                'b': 'narrative_support',
                'c': 'dramatic_punctuation'
            }
            usage = user_responses['usage']
            enhanced_classification['intensity'] = usage_map.get(usage, usage)
        
        if 'tags' in user_responses:
            enhanced_classification['tags'] = user_responses['tags']
        
        if 'semantic_description' in user_responses:
            # Use semantic description for enhanced filename
            semantic = user_responses['semantic_description']
            enhanced_classification['suggested_filename'] = self.enhanced_semantic_extraction(semantic)
        
        # Mark as user-enhanced with high confidence
        enhanced_classification['confidence'] = 0.95
        enhanced_classification['user_enhanced'] = True
        enhanced_classification['enhancement_method'] = 'interactive_with_audio'
        
        return enhanced_classification
    
    def modify_classification(self, classification):
        """Allow user to modify AI classification."""
        print("\nModify classification (press Enter to keep current value):")
        
        new_classification = classification.copy()
        
        category = input(f"Category [{classification.get('category', '')}]: ").strip()
        if category:
            new_classification['category'] = category
        
        mood = input(f"Mood [{classification.get('mood', '')}]: ").strip()
        if mood:
            new_classification['mood'] = mood
        
        intensity = input(f"Intensity [{classification.get('intensity', '')}]: ").strip()
        if intensity:
            new_classification['intensity'] = intensity
        
        # Mark as user-corrected with high confidence
        new_classification['confidence'] = 0.95
        new_classification['user_corrected'] = True
        
        return new_classification
    
    def process_file_interactive(self, file_path, dry_run=True):
        """Process file with interactive classification."""
        file_path = Path(file_path)
        print(f"\nüîç Processing: {file_path.name}")
        
        # Get initial classification
        classification = self.classify_audio_file(file_path)
        
        if classification:
            # Check if user input needed
            if self.should_ask_user(classification):
                print(f"  ü§ñ AI confidence: {classification.get('confidence', 0):.1%} (below {self.interaction_threshold:.1%} threshold)")
                classification = self.ask_user_for_clarification(file_path, classification)
            
            if classification:
                target_folder = self.determine_target_folder(classification)
                suggested_filename = classification.get('suggested_filename', '')
                
                print(f"  üìÇ Category: {classification.get('category', 'unknown')}")
                print(f"  üé≠ Mood: {classification.get('mood', 'unknown')}")
                print(f"  ‚ö° Intensity: {classification.get('intensity', 'unknown')}")
                print(f"  üî• Energy: {classification.get('energy_level', 0)}/10")
                print(f"  üéØ Confidence: {classification.get('confidence', 0):.1%}")
                print(f"  üìç Target: {target_folder}")
                print(f"  üè∑Ô∏è Tags: {', '.join(classification.get('tags', []))}")
                
                if classification.get('discovered_elements'):
                    print(f"  üÜï Discovered: {', '.join(classification.get('discovered_elements', []))}")
                
                if not dry_run:
                    new_path = self.move_file_safely(file_path, target_folder, suggested_filename)
                    if new_path:
                        print(f"  ‚úÖ Moved to: {new_path}")
                        classification['final_path'] = str(new_path)
                    else:
                        print(f"  ‚ùå Failed to move")
                else:
                    print(f"  üîÑ [DRY RUN] Would move to: {target_folder}")
                
                return classification
            else:
                print(f"  ‚è≠Ô∏è Skipped by user")
                return None
        else:
            print(f"  ‚ùå Classification failed")
            return None
    
    def find_audio_files_recursive(self, directory):
        """Find all audio files in directory recursively."""
        audio_files = []
        directory = Path(directory)
        
        if not directory.exists():
            print(f"‚ö†Ô∏è Directory not found: {directory}")
            return audio_files
        
        for file_path in directory.rglob('*'):
            if file_path.is_file() and file_path.suffix.lower() in self.audio_extensions:
                audio_files.append(str(file_path))
        
        return audio_files
    
    def interactive_batch_process(self, file_list, confidence_threshold=0.7, dry_run=True):
        """Process multiple files with smart interaction."""
        if not file_list:
            print("‚ùå No files provided for processing")
            return []
        
        # Set confidence threshold for this batch
        original_threshold = self.interaction_threshold
        self.interaction_threshold = confidence_threshold
        
        print(f"\nüéµ Starting interactive batch processing...")
        print(f"üìÅ Files to process: {len(file_list)}")
        print(f"üéØ Confidence threshold: {confidence_threshold:.1%}")
        print(f"üîÑ Dry run: {dry_run}")
        print("=" * 50)
        
        results = []
        processed_count = 0
        skipped_count = 0
        high_confidence_count = 0
        interactive_count = 0
        
        try:
            iterator = tqdm(file_list, desc="Processing audio files") if TQDM_AVAILABLE else file_list
            
            for i, file_path in enumerate(iterator):
                try:
                    print(f"\n[{i+1}/{len(file_list)}] Processing: {Path(file_path).name}")
                    
                    # Get initial AI classification to check confidence
                    initial_classification = self.classify_audio_file(file_path)
                    
                    if initial_classification:
                        confidence = initial_classification.get('confidence', 0)
                        
                        if confidence >= confidence_threshold:
                            # High confidence - process automatically
                            print(f"ü§ñ High confidence ({confidence:.1%}) - processing automatically")
                            result = self.process_file_automatically(file_path, initial_classification, dry_run)
                            high_confidence_count += 1
                        else:
                            # Low confidence - interactive processing
                            print(f"ü§î Low confidence ({confidence:.1%}) - requesting user input")
                            result = self.process_file_interactive(file_path, dry_run=dry_run)
                            interactive_count += 1
                        
                        if result:
                            results.append(result)
                            processed_count += 1
                        else:
                            skipped_count += 1
                    else:
                        print(f"‚ùå Classification failed - skipping")
                        skipped_count += 1
                    
                    # Small delay to avoid hitting API limits
                    time.sleep(0.5)
                    
                except KeyboardInterrupt:
                    print("\n‚èπÔ∏è Batch processing interrupted by user")
                    break
                except Exception as e:
                    print(f"‚ùå Error processing {file_path}: {e}")
                    skipped_count += 1
        
        finally:
            # Restore original threshold
            self.interaction_threshold = original_threshold
        
        print(f"\n{'='*50}")
        print(f"üéâ Interactive batch processing completed!")
        print(f"‚úÖ Processed: {processed_count}")
        print(f"ü§ñ Auto-processed (high confidence): {high_confidence_count}")
        print(f"ü§î Interactive (needed help): {interactive_count}")
        print(f"‚è≠Ô∏è Skipped: {skipped_count}")
        print(f"üìä Total: {len(file_list)}")
        print(f"üìà Success rate: {processed_count/len(file_list):.1%}")
        
        return results
    
    def process_file_automatically(self, file_path, classification, dry_run=True):
        """Process file automatically with existing classification."""
        file_path = Path(file_path)
        
        if classification:
            target_folder = self.determine_target_folder(classification)
            suggested_filename = classification.get('suggested_filename', '')
            
            print(f"  üìÇ Category: {classification.get('category', 'unknown')}")
            print(f"  üé≠ Mood: {classification.get('mood', 'unknown')}")
            print(f"  ‚ö° Intensity: {classification.get('intensity', 'unknown')}")
            print(f"  üî• Energy: {classification.get('energy_level', 0)}/10")
            print(f"  üéØ Confidence: {classification.get('confidence', 0):.1%}")
            print(f"  üìç Target: {target_folder}")
            
            if not dry_run:
                new_path = self.move_file_safely(file_path, target_folder, suggested_filename)
                if new_path:
                    print(f"  ‚úÖ Moved to: {new_path}")
                    classification['final_path'] = str(new_path)
                else:
                    print(f"  ‚ùå Failed to move")
            else:
                print(f"  üîÑ [DRY RUN] Would move to: {target_folder}")
            
            return classification
        
        return None
    
    def create_metadata_spreadsheet(self):
        """Create comprehensive metadata spreadsheet."""
        print("üìä Creating metadata spreadsheet...")
        
        # Collect all processed files
        all_data = []
        
        for classification in self.learning_data['classifications']:
            row = {
                'original_filename': classification.get('filename', ''),
                'category': classification.get('category', ''),
                'mood': classification.get('mood', ''),
                'intensity': classification.get('intensity', ''),
                'energy_level': classification.get('energy_level', 0),
                'confidence': classification.get('confidence', 0),
                'timestamp': classification.get('timestamp', ''),
                'user_corrected': classification.get('user_corrected', False),
                'final_path': classification.get('final_path', ''),
                'bpm': classification.get('bpm', 0),
                'tags': ', '.join(classification.get('tags', [])) if classification.get('tags') else ''
            }
            all_data.append(row)
        
        if not all_data:
            print("‚ö†Ô∏è No classification data found")
            return None, None
        
        # Create DataFrame
        df = pd.DataFrame(all_data)
        
        # Add analysis columns
        df['confidence_category'] = df['confidence'].apply(
            lambda x: 'High' if x >= 0.8 else 'Medium' if x >= 0.5 else 'Low'
        )
        
        df['processing_date'] = pd.to_datetime(df['timestamp']).dt.date
        
        # Sort by most recent first
        df = df.sort_values('timestamp', ascending=False)
        
        # Save to Excel with multiple sheets
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        spreadsheet_path = self.base_dir / "04_METADATA_SYSTEM" / f"audio_metadata_{timestamp}.xlsx"
        
        with pd.ExcelWriter(spreadsheet_path, engine='openpyxl') as writer:
            # Main data sheet
            df.to_excel(writer, sheet_name='All_Files', index=False)
            
            # Summary by category
            category_summary = df.groupby('category').agg({
                'original_filename': 'count',
                'confidence': 'mean',
                'energy_level': 'mean'
            }).round(2)
            category_summary.columns = ['File_Count', 'Avg_Confidence', 'Avg_Energy']
            category_summary.to_excel(writer, sheet_name='Category_Summary')
            
            # Summary by mood
            mood_summary = df.groupby('mood').agg({
                'original_filename': 'count',
                'confidence': 'mean',
                'energy_level': 'mean'
            }).round(2)
            mood_summary.columns = ['File_Count', 'Avg_Confidence', 'Avg_Energy']
            mood_summary.to_excel(writer, sheet_name='Mood_Summary')
            
            # Learning statistics
            learning_stats = pd.DataFrame([{
                'Total_Files_Processed': len(df),
                'High_Confidence_Files': len(df[df['confidence'] >= 0.8]),
                'User_Corrected_Files': len(df[df['user_corrected'] == True]),
                'Unique_Categories': df['category'].nunique(),
                'Unique_Moods': df['mood'].nunique(),
                'Avg_Overall_Confidence': df['confidence'].mean()
            }])
            learning_stats.to_excel(writer, sheet_name='Learning_Stats', index=False)
        
        print(f"üìà Metadata spreadsheet saved: {spreadsheet_path}")
        print(f"üìã Sheets: All_Files, Category_Summary, Mood_Summary, Learning_Stats")
        
        return df, spreadsheet_path
    
    def sweep_system(self, directories_to_scan, dry_run=True):
        """Sweep multiple directories for audio files and process them."""
        print(f"üßπ Starting system sweep...")
        print(f"üìÅ Base directory: {self.base_dir}")
        print(f"üîÑ Dry run mode: {dry_run}")
        print("-" * 50)
        
        all_files = []
        for directory in directories_to_scan:
            print(f"üîç Scanning: {directory}")
            files = self.find_audio_files_recursive(directory)
            all_files.extend(files)
            print(f"  üìÑ Found {len(files)} audio files")
        
        print(f"\nüìä Total audio files found: {len(all_files)}")
        
        if not all_files:
            print("‚ùå No audio files to process!")
            return []
        
        # Process files in batch
        results = self.interactive_batch_process(all_files, dry_run=dry_run)
        
        # Generate metadata spreadsheet
        if results:
            df, spreadsheet_path = self.create_metadata_spreadsheet()
            print(f"üìà Complete metadata available at: {spreadsheet_path}")
        
        return results
    
    def show_learning_stats(self):
        """Display learning statistics."""
        print(f"\nüìä AUDIOAI LEARNING STATISTICS")
        print(f"=" * 50)
        
        total_files = len(self.learning_data['classifications'])
        print(f"üéµ Total files processed: {total_files}")
        
        if total_files > 0:
            # Confidence stats
            confidences = [c.get('confidence', 0) for c in self.learning_data['classifications']]
            avg_confidence = sum(confidences) / len(confidences)
            high_confidence = len([c for c in confidences if c >= 0.8])
            
            print(f"üéØ Average confidence: {avg_confidence:.1%}")
            print(f"‚≠ê High confidence files: {high_confidence}/{total_files} ({high_confidence/total_files:.1%})")
        
        print(f"\nüÜï DISCOVERED CATEGORIES:")
        new_categories = self.discovered_categories.get('new_categories', [])
        if new_categories:
            for category in new_categories:
                print(f"  - {category}")
        else:
            print("  (none yet)")
        
        print(f"\nüé≠ DISCOVERED MOODS:")
        new_moods = self.discovered_categories.get('new_moods', [])
        if new_moods:
            for mood in new_moods:
                print(f"  - {mood}")
        else:
            print("  (none yet)")
        
        print(f"\nüìà MOST COMMON COMBINATIONS:")
        freq_counts = self.discovered_categories.get('frequency_counts', {})
        if freq_counts:
            for combo, count in sorted(freq_counts.items(), key=lambda x: x[1], reverse=True)[:10]:
                print(f"  - {combo}: {count} files")
        else:
            print("  (no data yet)")
        
        print(f"\nüîç FILENAME PATTERNS LEARNED:")
        patterns = self.learning_data['filename_patterns']
        if patterns:
            for pattern, examples in list(patterns.items())[:5]:
                print(f"  - {pattern}: {len(examples)} examples")
        else:
            print("  (none yet)")
        
        # User corrections
        user_corrections = len([c for c in self.learning_data['classifications'] if c.get('user_corrected')])
        if user_corrections > 0:
            print(f"\n‚úèÔ∏è USER CORRECTIONS: {user_corrections} files")
            print("  (System learning from your feedback!)")
    
    def export_learning_data(self):
        """Export learning data for backup or sharing."""
        export_data = {
            'learning_data': self.learning_data,
            'discovered_categories': self.discovered_categories,
            'folder_map': self.folder_map,
            'export_timestamp': datetime.now().isoformat()
        }
        
        export_path = self.base_dir / "04_METADATA_SYSTEM" / f"learning_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(export_path, 'w') as f:
            json.dump(export_data, f, indent=2, default=str)
        
        print(f"üì§ Learning data exported to: {export_path}")
        return export_data
    
    def import_learning_data(self, import_path):
        """Import learning data from backup."""
        try:
            with open(import_path, 'r') as f:
                import_data = json.load(f)
            
            # Merge learning data
            if 'learning_data' in import_data:
                existing_classifications = self.learning_data['classifications']
                imported_classifications = import_data['learning_data']['classifications']
                
                # Avoid duplicates based on filename + timestamp
                existing_keys = {(c.get('filename', ''), c.get('timestamp', '')) for c in existing_classifications}
                
                new_classifications = [
                    c for c in imported_classifications 
                    if (c.get('filename', ''), c.get('timestamp', '')) not in existing_keys
                ]
                
                self.learning_data['classifications'].extend(new_classifications)
                print(f"üì• Imported {len(new_classifications)} new classifications")
            
            # Merge discovered categories
            if 'discovered_categories' in import_data:
                imported_cats = import_data['discovered_categories']
                
                for cat in imported_cats.get('new_categories', []):
                    if cat not in self.discovered_categories['new_categories']:
                        self.discovered_categories['new_categories'].append(cat)
                
                for mood in imported_cats.get('new_moods', []):
                    if mood not in self.discovered_categories['new_moods']:
                        self.discovered_categories['new_moods'].append(mood)
                
                print(f"üì• Merged discovered categories and moods")
            
            # Save merged data
            self.save_learning_data()
            self.save_discovered_categories()
            
            # Rebuild folder map
            self.folder_map = self.build_dynamic_folder_map()
            
            print(f"‚úÖ Learning data import completed successfully")
            
        except Exception as e:
            print(f"‚ùå Failed to import learning data: {e}")
    
    def add_custom_categories(self, custom_categories):
        """Add custom category definitions."""
        for category, moods in custom_categories.items():
            # Add to base categories
            self.base_categories[category] = moods
            
            # Add to discovered categories to make them available immediately
            if category not in self.discovered_categories['new_categories']:
                self.discovered_categories['new_categories'].append(category)
            
            for mood in moods:
                if mood not in self.discovered_categories['new_moods']:
                    self.discovered_categories['new_moods'].append(mood)
        
        # Rebuild folder map with new categories
        self.folder_map = self.build_dynamic_folder_map()
        
        # Save the updates
        self.save_discovered_categories()
        
        print(f"‚úÖ Added {len(custom_categories)} custom categories")
        for category, moods in custom_categories.items():
            print(f"  - {category}: {', '.join(moods)}")
    
    def enhanced_semantic_extraction(self, original_name):
        """Extract semantic information preserving original meaning."""
        import re
        
        name = original_name
        
        # Remove technical junk but keep meaningful content
        junk_patterns = [
            r'ES_',
            r'gen_sp\d+_s\d+_sb\d+_se\d+_b_m\d+',
            r'\d{4}-\d{2}-\d{2}T\d{2}_\d{2}_\d{2}',
            r'_gen_sp\d+.*
def organize_music_library(api_key, library_path, source_directories, dry_run=True):
    """Quick setup for music producers."""
    organizer = AdaptiveAudioOrganizer(api_key, library_path)
    
    # Music-specific categories
    music_categories = {
        "drums_acoustic": ["kick", "snare", "hihat", "cymbal", "percussion"],
        "drums_electronic": ["808", "trap", "techno", "house", "experimental"],
        "bass_synth": ["reese", "wobble", "sub", "lead", "pluck"],
        "leads_analog": ["warm", "vintage", "bright", "aggressive"],
        "pads_ambient": ["lush", "dark", "ethereal", "rhythmic"],
        "fx_transitions": ["sweep", "riser", "impact", "reverse"]
    }
    
    organizer.add_custom_categories(music_categories)
    organizer.set_interaction_mode('minimal')  # Less interruption during creative flow
    
    return organizer.sweep_system(source_directories, dry_run=dry_run)


def organize_podcast_library(api_key, library_path, source_directories, dry_run=True):
    """Quick setup for podcast creators."""
    organizer = AdaptiveAudioOrganizer(api_key, library_path)
    
    # Podcast-specific categories
    podcast_categories = {
        "voice_host": ["intro", "outro", "transition", "interview"],
        "voice_guest": ["interview", "soundbite", "quote"],
        "music_intro": ["energetic", "branded", "welcoming"],
        "music_background": ["subtle", "neutral", "supportive"],
        "sfx_transition": ["swoosh", "chime", "pause"],
        "sfx_emphasis": ["alert", "notification", "highlight"]
    }
    
    organizer.add_custom_categories(podcast_categories)
    organizer.set_interaction_mode('always')  # Maximum precision for podcast work
    
    return organizer.sweep_system(source_directories, dry_run=dry_run)


# Main execution example
if __name__ == "__main__":
    # Configuration
    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY') or "your-api-key-here"
    BASE_DIRECTORY = "/path/to/your/audio/library"
    
    # Directories to scan (customize these paths)
    DIRECTORIES_TO_SCAN = [
        "/Users/username/Downloads",
        "/Users/username/Desktop",
        "/Users/username/Documents/Audio",
        # Add your actual directories here
    ]
    
    # Initialize organizer
    organizer = AdaptiveAudioOrganizer(OPENAI_API_KEY, BASE_DIRECTORY)
    
    # Show current learning state
    organizer.show_learning_stats()
    
    # Example: Process a single file first
    # result = organizer.process_file_interactive("test_audio.mp3", dry_run=True)
    
    # Example: Run full system sweep
    # results = organizer.sweep_system(DIRECTORIES_TO_SCAN, dry_run=True)
    
    print("\nüéµ AudioAI Organizer ready!")
    print("üìñ See README.md for detailed usage examples")
    print("üöÄ Start with dry_run=True to preview before moving files")
                ,
        ]
        
        for pattern in junk_patterns:
            name = re.sub(pattern, '', name, flags=re.IGNORECASE)
        
        # Remove BPM since librosa will detect it
        name = re.sub(r'\d+\s*bpm\s*', '', name, flags=re.IGNORECASE)
        
        # Determine content type
        content_type = 'unknown'
        if 'out-of-breath' in name.lower() or 'breathing' in name.lower() or 'heartbeat' in name.lower():
            content_type = 'sfx_human'
        elif 'voice' in name.lower() or (any(word in name.lower() for word in ['male', 'female', 'narrator', 'accent']) and 'beat' not in name.lower()):
            content_type = 'voice'
        elif any(word in name.lower() for word in ['beat', 'rhythm', 'song', 'track', 'music', 'instrumental']):
            content_type = 'music'
        elif any(word in name.lower() for word in ['sfx', 'sound', 'effect', 'ambient', 'drone', 'signals', 'digital']):
            content_type = 'sfx'
        
        if content_type == 'voice':
            return self._extract_voice_semantics(name)
        elif content_type == 'sfx_human':
            return self._extract_sfx_human_semantics(name)
        elif content_type == 'music':
            return self._extract_music_semantics(name)
        elif content_type == 'sfx':
            return self._extract_sfx_semantics(name)
        else:
            return self._extract_general_semantics(name)
    
    def _extract_voice_semantics(self, name):
        """Extract voice-specific semantic information."""
        components = []
        
        # Handle compound descriptors
        if 'uk-asian' in name.lower():
            components.append('UK-Asian')
        elif 'uk' in name.lower() or 'british' in name.lower():
            components.append('UK')
        elif 'asian' in name.lower():
            components.append('Asian')
        
        # Age/Gender
        age_gender = []
        if 'young' in name.lower():
            age_gender.append('Young')
        if 'female' in name.lower():
            age_gender.append('Female')
        elif 'male' in name.lower():
            age_gender.append('Male')
        
        if len(age_gender) > 1:
            components.append('_'.join(age_gender))
        else:
            components.extend(age_gender)
        
        # Voice characteristics
        characteristics = ['attractive', 'deep', 'gruff', 'clear', 'raspy']
        for char in characteristics:
            if char.lower() in name.lower():
                components.append(char.title())
        
        components.append('Voice')
        
        # Numbers
        numbers = re.findall(r'\b(\d{2,3})\b', name)
        if numbers:
            components.append(numbers[0])
        
        return '_'.join(components)
    
    def _extract_sfx_human_semantics(self, name):
        """Extract human SFX semantics."""
        components = []
        
        if 'female' in name.lower():
            components.append('Female')
        elif 'male' in name.lower():
            components.append('Male')
        
        if 'out-of-breath' in name.lower():
            components.append('Out-of-Breath')
        elif 'breathing' in name.lower():
            components.append('Breathing')
        elif 'heartbeat' in name.lower():
            components.append('Heartbeat')
        
        components.append('SFX')
        
        numbers = re.findall(r'\b(\d{3,6})\b', name)
        if numbers:
            components.append(numbers[0])
        
        return '_'.join(components)
    
    def _extract_music_semantics(self, name):
        """Extract music-specific semantic information."""
        components = []
        
        name = re.sub(r'^es_|^soundbed', '', name, flags=re.IGNORECASE)
        
        descriptive_words = [
            'playful', 'childlike', 'pulsing', 'signals', 'digital', 'space',
            'ambient', 'atmospheric', 'cinematic', 'touching', 'magical'
        ]
        
        words = re.split(r'[_\s\-]+', name)
        descriptors = []
        
        for word in words:
            if word.lower() in [d.lower() for d in descriptive_words]:
                descriptors.append(word.lower())
        
        components.extend(descriptors)
        
        # Extract title and artist
        artist_match = re.search(r'\s*-\s*([^-]+?)(?:\.|$)', name)
        if artist_match:
            title_part = name[:artist_match.start()].strip()
            artist_part = artist_match.group(1).strip()
            
            title_clean = re.sub(r'^(es[\s_]*|soundbed[\s_]*)', '', title_part, flags=re.IGNORECASE)
            for desc in descriptors:
                title_clean = re.sub(rf'\b{re.escape(desc)}\b', '', title_clean, flags=re.IGNORECASE)
            
            title_clean = re.sub(r'\s+', ' ', title_clean).strip()
            if title_clean:
                components.append(title_clean)
            
            artist_words = artist_part.split()
            if artist_words:
                components.append(artist_words[0])
        
        return '_'.join(components)
    
    def _extract_sfx_semantics(self, name):
        """Extract SFX semantics."""
        components = []
        
        descriptors = ['pulsing', 'signals', 'digital', 'space', 'mechanical', 'ambient']
        skip_words = {'es', 'from', 'the', 'and'}
        
        words = re.split(r'[_\s\-]+', name)
        
        for word in words:
            word_clean = word.lower().strip()
            if word_clean and word_clean not in skip_words:
                if word_clean in [d.lower() for d in descriptors]:
                    components.append(word_clean)
                elif len(word_clean) > 2:
                    components.append(word_clean)
        
        return '_'.join(components[:4])  # Limit to 4 components
    
    def _extract_general_semantics(self, name):
        """Extract general semantic information."""
        components = []
        
        # Remove common prefixes
        name = re.sub(r'^(es_|soundbed_)', '', name, flags=re.IGNORECASE)
        
        # Split and clean
        words = re.split(r'[_\s\-]+', name)
        meaningful_words = []
        
        skip_words = {'es', 'the', 'and', 'of', 'in', 'at', 'to', 'for', 'with'}
        
        for word in words:
            word_clean = word.lower().strip()
            if word_clean and word_clean not in skip_words and len(word_clean) > 1:
                if not word_clean.isdigit() or len(word_clean) > 3:
                    meaningful_words.append(word_clean)
        
        return '_'.join(meaningful_words[:4])  # Limit to 4 words
    
    def create_file_aliases(self, file_path, classification):
        """Create symbolic links/aliases in multiple relevant folders."""
        if not classification:
            return
        
        # Determine all possible target folders for this classification
        targets = self.determine_all_target_folders(classification)
        
        for target_folder in targets:
            target_dir = self.base_dir / target_folder
            target_dir.mkdir(parents=True, exist_ok=True)
            
            # Create alias filename
            original_name = Path(file_path).name
            alias_path = target_dir / original_name
            
            try:
                # On Unix systems, create symbolic link
                if not alias_path.exists():
                    alias_path.symlink_to(file_path)
                    print(f"üîó Created alias: {alias_path}")
            except (OSError, NotImplementedError):
                # On Windows or if symlinks not supported, copy file
                if not alias_path.exists():
                    shutil.copy2(file_path, alias_path)
                    print(f"üìÑ Created copy: {alias_path}")
    
    def determine_all_target_folders(self, classification):
        """Find all relevant folders for a classification."""
        targets = []
        
        category = classification.get('category', '')
        mood = classification.get('mood', '')
        intensity = classification.get('intensity', '')
        tags = classification.get('tags', [])
        
        # Primary target
        primary_target = self.determine_target_folder(classification)
        targets.append(primary_target)
        
        # Thematic collections based on tags
        thematic_targets = []
        for tag in tags:
            if 'consciousness' in tag.lower() or 'awareness' in tag.lower():
                thematic_targets.append("THEMATIC_COLLECTIONS/digital_consciousness/")
            elif 'human' in tag.lower() or 'dialogue' in tag.lower():
                thematic_targets.append("THEMATIC_COLLECTIONS/human_machine_dialogue/")
            elif 'memory' in tag.lower() or 'formation' in tag.lower():
                thematic_targets.append("THEMATIC_COLLECTIONS/memory_formation/")
            elif 'emergence' in tag.lower() or 'awakening' in tag.lower():
                thematic_targets.append("THEMATIC_COLLECTIONS/emergence_awakening/")
        
        targets.extend(thematic_targets)
        
        # Remove duplicates
        return list(set(targets))
    
    def filename_similarity(self, filename1, filename2):
        """Calculate similarity between filenames."""
        # Simple word-based similarity
        words1 = set(filename1.lower().replace('_', ' ').split())
        words2 = set(filename2.lower().replace('_', ' ').split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union)
    
    def group_files_by_tempo(self, file_list):
        """Group audio files by BPM ranges for efficient processing."""
        bpm_groups = defaultdict(list)
        
        for file_path in file_list:
            try:
                # Quick BPM detection
                y, sr = librosa.load(file_path, duration=30)
                tempo, _ = librosa.beat.beat_track(y=y, sr=sr)
                bpm = float(tempo) if np.isscalar(tempo) else float(tempo[0])
                
                # Group by BPM ranges
                if bpm < 80:
                    bmp_groups["slow (60-80)"].append(file_path)
                elif bpm < 120:
                    bpm_groups["moderate (80-120)"].append(file_path)
                elif bpm < 160:
                    bpm_groups["fast (120-160)"].append(file_path)
                else:
                    bpm_groups["very fast (160+)"].append(file_path)
                    
            except Exception as e:
                bpm_groups["unknown tempo"].append(file_path)
        
        return dict(bpm_groups)
    
    def find_compatible_keys(self, key):
        """Find musically compatible keys for harmonic mixing."""
        # Basic music theory compatibility
        compatible_keys = {
            'C': ['G', 'F', 'Am', 'Em'],
            'G': ['D', 'C', 'Em', 'Bm'],
            'D': ['A', 'G', 'Bm', 'F#m'],
            'A': ['E', 'D', 'F#m', 'C#m'],
            'E': ['B', 'A', 'C#m', 'G#m'],
            'B': ['F#', 'E', 'G#m', 'D#m'],
            'F#': ['C#', 'B', 'D#m', 'A#m'],
            'C#': ['G#', 'F#', 'A#m', 'Fm'],
            'F': ['C', 'Bb', 'Dm', 'Am'],
            'Bb': ['F', 'Eb', 'Gm', 'Dm'],
            'Eb': ['Bb', 'Ab', 'Cm', 'Gm'],
            'Ab': ['Eb', 'Db', 'Fm', 'Cm']
        }
        
        return compatible_keys.get(key, [])


# Convenience functions for common workflows
def organize_music_library(api_key, library_path, source_directories, dry_run=True):
    """Quick setup for music producers."""
    organizer = AdaptiveAudioOrganizer(api_key, library_path)
    
    # Music-specific categories
    music_categories = {
        "drums_acoustic": ["kick", "snare", "hihat", "cymbal", "percussion"],
        "drums_electronic": ["808", "trap", "techno", "house", "experimental"],
        "bass_synth": ["reese", "wobble", "sub", "lead", "pluck"],
        "leads_analog": ["warm", "vintage", "bright", "aggressive"],
        "pads_ambient": ["lush", "dark", "ethereal", "rhythmic"],
        "fx_transitions": ["sweep", "riser", "impact", "reverse"]
    }
    
    organizer.add_custom_categories(music_categories)
    organizer.set_interaction_mode('minimal')  # Less interruption during creative flow
    
    return organizer.sweep_system(source_directories, dry_run=dry_run)


def organize_podcast_library(api_key, library_path, source_directories, dry_run=True):
    """Quick setup for podcast creators."""
    organizer = AdaptiveAudioOrganizer(api_key, library_path)
    
    # Podcast-specific categories
    podcast_categories = {
        "voice_host": ["intro", "outro", "transition", "interview"],
        "voice_guest": ["interview", "soundbite", "quote"],
        "music_intro": ["energetic", "branded", "welcoming"],
        "music_background": ["subtle", "neutral", "supportive"],
        "sfx_transition": ["swoosh", "chime", "pause"],
        "sfx_emphasis": ["alert", "notification", "highlight"]
    }
    
    organizer.add_custom_categories(podcast_categories)
    organizer.set_interaction_mode('always')  # Maximum precision for podcast work
    
    return organizer.sweep_system(source_directories, dry_run=dry_run)


# Quick setup functions for different workflows
def setup_podcast_organizer(api_key, library_path):
    """Quick setup for podcast creators with consciousness themes."""
    organizer = AdaptiveAudioOrganizer(api_key, library_path)
    
    # Podcast-specific categories
    consciousness_categories = {
        "music_consciousness": ["contemplative", "awakening", "digital_emergence", "memory_formation", "wonder_discovery"],
        "sfx_consciousness": ["data_processing", "thought_formation", "digital_heartbeat", "memory_access", "attention_mechanism"],
        "sfx_human_ai": ["dialogue_bridge", "connection_moment", "understanding_dawn", "barrier_dissolve"],
        "voice_ai": ["synthetic_warmth", "digital_personality", "consciousness_voice", "memory_echo"],
        "voice_human": ["wonder", "fear", "acceptance", "connection", "discovery"],
        "ambient_liminal": ["threshold_space", "between_worlds", "transformation", "emergence"],
        "attention_themed": ["focus_shift", "pattern_recognition", "information_flow", "neural_attention", "transformer_process"]
    }
    
    organizer.add_custom_categories(consciousness_categories)
    organizer.set_interaction_mode('minimal')  # ADHD-friendly
    
    return organizer


def suggest_cues_for_scene(organizer, scene_description, max_suggestions=5):
    """Get cue suggestions for a specific scene."""
    print(f"üé¨ Scene: {scene_description}")
    print(f"üîç Searching organized library...")
    
    matches = []
    scene_words = scene_description.lower().split()
    
    # Search through learning data for matches
    for classification_entry in organizer.learning_data['classifications']:
        classification = classification_entry.get('classification', {})
        filename = classification_entry.get('filename', '')
        
        relevance = 0
        
        # Check mood matching
        mood = classification.get('mood', '').lower()
        for word in scene_words:
            if word in mood:
                relevance += 3
        
        # Check tags
        tags = classification.get('tags', [])
        for tag in tags:
            for word in scene_words:
                if word in tag.lower():
                    relevance += 2
        
        # Check reasoning
        reasoning = classification.get('reasoning', '').lower()
        for word in scene_words:
            if word in reasoning:
                relevance += 1
        
        # Special attention-themed matches
        if any(attention_word in scene_description.lower() for attention_word in 
               ['attention', 'focus', 'transform', 'pattern', 'neural', 'process']):
            if any(attention_word in str(classification).lower() for attention_word in 
                   ['attention', 'focus', 'neural', 'process', 'digital', 'transform']):
                relevance += 4
        
        if relevance > 0:
            matches.append({
                'filename': filename,
                'relevance': relevance,
                'mood': classification.get('mood', 'unknown'),
                'energy': classification.get('energy_level', 0),
                'intensity': classification.get('intensity', 'unknown'),
                'tags': classification.get('tags', []),
                'reasoning': classification.get('reasoning', ''),
                'confidence': classification.get('confidence', 0),
                'category': classification.get('category', 'unknown'),
                'bpm': classification.get('bpm', 0)
            })
    
    # Sort by relevance
    matches.sort(key=lambda x: x['relevance'], reverse=True)
    
    # Display suggestions
    if matches:
        print(f"‚ú® Found {len(matches)} potential matches:")
        print()
        
        for i, match in enumerate(matches[:max_suggestions], 1):
            print(f"{i}. üéµ {match['filename']}")
            print(f"   üé≠ {match['mood']} | ‚ö° Energy: {match['energy']}/10 | üéØ {match['intensity']}")
            if match['bpm'] > 0:
                print(f"   ü•Å {match['bpm']} BPM | üìÇ {match['category']}")
            else:
                print(f"   üìÇ {match['category']}")
            print(f"   üè∑Ô∏è Tags: {', '.join(match['tags'][:4])}")
            print(f"   üí≠ Why: {match['reasoning'][:120]}...")
            print(f"   üìä Relevance: {match['relevance']} | üî• Confidence: {match['confidence']:.1%}")
            print()
    else:
        print("ü§î No matches found for that description.")
        print("üí° Try describing differently or check if more audio needs organizing!")
    
    return matches[:max_suggestions]


# Main execution example
if __name__ == "__main__":
    # Configuration
    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY') or "your-api-key-here"
    BASE_DIRECTORY = "/path/to/your/audio/library"
    
    # Initialize organizer
    organizer = setup_podcast_organizer(OPENAI_API_KEY, BASE_DIRECTORY)
    
    # Show current learning state
    organizer.show_learning_stats()
    
    print("\nüéµ AudioAI Organizer ready!")
    print("üöÄ Enhanced with voice detection and consciousness themes!")
    print("üìñ Save this file as 'audioai_organizer.py' to preserve all fixes")